{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "\n",
    "class Model(tf.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE), name='flatten'),\n",
    "        tf.keras.layers.Dense(128, activation='relu', name='dense_1'),\n",
    "        tf.keras.layers.Dense(10, name='dense_2')\n",
    "    ])\n",
    "\n",
    "    self.model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "  # The `train` function takes a batch of input images and labels.\n",
    "  @tf.function(input_signature=[\n",
    "      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\n",
    "      tf.TensorSpec([None, 10], tf.float32),\n",
    "  ])\n",
    "  def train(self, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      prediction = self.model(x)\n",
    "      loss = self.model.loss(y, prediction)\n",
    "    gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "    self.model.optimizer.apply_gradients(\n",
    "        zip(gradients, self.model.trainable_variables))\n",
    "    result = {\"loss\": loss}\n",
    "    return result\n",
    "\n",
    "  @tf.function(input_signature=[\n",
    "      tf.TensorSpec([None, IMG_SIZE, IMG_SIZE], tf.float32),\n",
    "  ])\n",
    "  def infer(self, x):\n",
    "    logits = self.model(x)\n",
    "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"output\": probabilities,\n",
    "        \"logits\": logits\n",
    "    }\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "  def save(self, checkpoint_path):\n",
    "    tensor_names = [weight.name for weight in self.model.weights]\n",
    "    tensors_to_save = [weight.read_value() for weight in self.model.weights]\n",
    "    tf.raw_ops.Save(\n",
    "        filename=checkpoint_path, tensor_names=tensor_names,\n",
    "        data=tensors_to_save, name='save')\n",
    "    return {\n",
    "        \"checkpoint_path\": checkpoint_path\n",
    "    }\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "  def restore(self, checkpoint_path):\n",
    "    restored_tensors = {}\n",
    "    for var in self.model.weights:\n",
    "      restored = tf.raw_ops.Restore(\n",
    "          file_pattern=checkpoint_path, tensor_name=var.name, dt=var.dtype,\n",
    "          name='restore')\n",
    "      var.assign(restored)\n",
    "      restored_tensors[var.name] = restored\n",
    "    return restored_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images = (train_images / 255.0).astype(np.float32)\n",
    "test_images = (test_images / 255.0).astype(np.float32)\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels).astype(np.float32)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 10 epochs\n",
      "  loss: 0.226\n",
      "Finished 20 epochs\n",
      "  loss: 0.186\n",
      "Finished 30 epochs\n",
      "  loss: 0.157\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 100\n",
    "epochs = np.arange(1, NUM_EPOCHS + 1, 1)\n",
    "losses = np.zeros([NUM_EPOCHS])\n",
    "m = Model()\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_ds = train_ds.batch(BATCH_SIZE)\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "  for x,y in train_ds:\n",
    "    result = m.train(x, y)\n",
    "\n",
    "  losses[i] = result['loss']\n",
    "  if (i + 1) % 10 == 0:\n",
    "    print(f\"Finished {i+1} epochs\")\n",
    "    print(f\"  loss: {losses[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_55304) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_55276) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "SAVED_MODEL_DIR = \"saved_model\"\n",
    "\n",
    "tf.saved_model.save(\n",
    "    m,\n",
    "    SAVED_MODEL_DIR,\n",
    "    signatures={\n",
    "        'train':\n",
    "            m.train.get_concrete_function(),\n",
    "        'infer':\n",
    "            m.infer.get_concrete_function(),\n",
    "        'save':\n",
    "            m.save.get_concrete_function(),\n",
    "        'restore':\n",
    "            m.restore.get_concrete_function(),\n",
    "    })\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "]\n",
    "converter.experimental_enable_resource_variables = True\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuned TFLite model saved as: models/orig_model_epochs_30_batch_100.tflite\n"
     ]
    }
   ],
   "source": [
    "tflite_model_path = f\"models/orig_model_epochs_{NUM_EPOCHS}_batch_{BATCH_SIZE}.tflite\"\n",
    "with open(tflite_model_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"✅ Fine-tuned TFLite model saved as: {tflite_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
